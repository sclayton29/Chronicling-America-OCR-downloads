{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting OCR files from Chronicling America in Bulk\n",
    "\n",
    "This jupyter notebook will allow you to download OCR files from [Chronicling Amercia](https://chroniclingamerica.loc.gov/), and Library of Congress database with historical digitized newspapers.\n",
    "\n",
    "* Go to [Chronicling Amercia](https://chroniclingamerica.loc.gov/) and do a search\n",
    "* Copy the URL from your search. You will need to enter it below. \n",
    "* Run the code below. Press Shift + Enter to Run the Cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code import the python libraries we need.\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "import re\n",
    "import csv\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search url form Chronicling America.https://chroniclingamerica.loc.gov/search/pages/results/?state=&date1=1925&date2=1925&proxtext=scopes+trial&x=19&y=15&dateFilterType=yearRange&rows=20&searchType=basic\n"
     ]
    }
   ],
   "source": [
    "#Enter your search URL\n",
    "search_url = input('Enter your search url form Chronicling America.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last page is 7 ...\n",
      "Saved all the page urls ...\n",
      "pulled the ids for all the relevant files...\n",
      "Saved metadata in CSV...\n",
      "Processed 50 of 137\n",
      "Processed 100 of 137\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "#This is the code that finds the URLs and downloads the OCR files as plain text. \n",
    "\n",
    "r = requests.get(search_url)\n",
    "soup=BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "pagination = soup.findAll('div',attrs={\"class\": \"left\"})\n",
    "\n",
    "#Finding the last page\n",
    "pages = []\n",
    "for div in pagination: \n",
    "    links = div.findAll('a')\n",
    "    for a in links:\n",
    "        pages.append(a.text)\n",
    "        \n",
    "last_page = int(pages[-2])\n",
    "print('The last page is', last_page, '...')\n",
    "\n",
    "#Saving all the page urls\n",
    "search_result_urls = []\n",
    "for number in range(last_page):\n",
    "    search_result_urls.append(search_url + '&page=' + str(number+1))\n",
    "print('Saved all the page urls ...')\n",
    "    \n",
    "#Getting all the links to OCR content\n",
    "ocr_links = []\n",
    "for url in search_result_urls:\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    ids = soup.findAll(attrs={\"name\": \"id\"})\n",
    "    for item in ids:\n",
    "        ocr_links.append('https://chroniclingamerica.loc.gov' + item['value'] + 'ocr.txt')\n",
    "print('pulled the ids for all the relevant files...')\n",
    "\n",
    "#Creating CSV with Metadata\n",
    "sn = []\n",
    "date = []\n",
    "filename = []\n",
    "for link in ocr_links:\n",
    "    sn.append(link[42:50])\n",
    "    date.append(link[51:61])\n",
    "    filename.append(link[40:].replace('/','_'))\n",
    "\n",
    "with open('metadata.csv','w') as outfile:\n",
    "    rowlists = zip(sn, date, filename)\n",
    "    writer = csv.writer(outfile)\n",
    "    for row in rowlists:\n",
    "        writer.writerows([row])\n",
    "print('Saved metadata in CSV...')\n",
    "\n",
    "#Creating and Saving Text Files\n",
    "n = 1\n",
    "for link in ocr_links:\n",
    "    req = requests.get(link).text.replace('\\n',' ').lower()\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "    req = pattern.sub('', req)\n",
    "    filename = link[40:].replace('/','_')\n",
    "    with open('text-files/' + filename, 'w') as f:\n",
    "        f.write(str(req))\n",
    "        if n % 50 == 0: #printing out progress status for every 10\n",
    "            print('Processed ' + str(n) + ' of ' + str(len(ocr_links)))\n",
    "    n= n + 1 \n",
    "print('DONE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
